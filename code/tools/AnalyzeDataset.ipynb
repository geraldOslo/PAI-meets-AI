{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dace864f-f553-493c-bc12-e9e2b6ec711c",
   "metadata": {},
   "source": [
    "# Dataset Analysis and Statistics for PAI Classification\n",
    "\n",
    "**Author:** Gerald Torgersen  \n",
    "**Date:** 2025  \n",
    "**GitHub:** [github.com/geraldOslo/PAI-meets-AI](https://github.com/geraldOslo/PAI-meets-AI)  \n",
    "\n",
    "**License**  \n",
    "SPDX-License-Identifier: MIT  \n",
    "Copyright (c) 2025 Gerald Torgersen\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook serves as a preliminary analysis tool for the Periapical Index (PAI) image dataset. Its primary purpose is to compute and display essential statistics required for configuring the main training pipeline. It performs two key tasks:\n",
    "\n",
    "1.  **Categorical Data Analysis**: It reads the dataset's metadata from CSV files and calculates the distribution (counts and percentages) of key features such as `quadrant`, `tooth`, `root`, and the target `PAI` score. This is crucial for understanding class imbalance and data characteristics.\n",
    "2.  **Image Pixel Statistics**: It processes the entire image set to compute the channel-wise `mean` and `standard deviation`. These values are essential for normalizing the input data in the training script, which helps stabilize training and improve model convergence.\n",
    "\n",
    "This notebook is designed to be run **before** model training. The outputs are intended to be copied into the `config.py` file used by the main training notebook.\n",
    "\n",
    "## Workflow Summary\n",
    "\n",
    "1.  **Configuration**: The user specifies paths to the metadata CSV files and the root directories containing the images in the \"USER CONFIGURATION\" cell. Image processing settings like target `IMAGE_SIZE` are also defined here.\n",
    "2.  **Metadata Loading**: The script loads and combines all specified CSV files into a single pandas DataFrame.\n",
    "3.  **Categorical Statistics**: It iterates through predefined columns of interest (`quadrant`, `tooth`, `root`, `PAI`) and prints detailed tables showing the frequency and percentage of each unique value.\n",
    "4.  **Image Path Aggregation**: The script constructs a complete list of absolute paths to all image files by combining the root directories and the filenames from the CSV. It also reports any missing files.\n",
    "5.  **Image Statistics Calculation**:\n",
    "    *   A custom PyTorch `Dataset` is used to load images efficiently.\n",
    "    *   Each image is resized to the specified `IMAGE_SIZE` and converted to a tensor.\n",
    "    *   The script iterates through all images in batches, calculating the sum and sum-of-squares of pixel values for each color channel (R, G, B) in a single pass. This is a numerically stable and efficient method.\n",
    "    *   Finally, it computes the overall `mean` and `standard deviation` from these accumulated values.\n",
    "6.  **Output**: All statistics are printed directly to the console. The user can then manually transfer the calculated `mean` and `std` values to their training configuration.\n",
    "\n",
    "## Key Analysis Techniques\n",
    "\n",
    "*   **Categorical Data Analysis**: Vital for identifying potential data entry issues (e.g., duplicate categories like `'B'` and `' B'`) and quantifying class imbalance. Understanding the PAI score distribution is the first step in deciding on strategies like oversampling, class weighting, or using specialized loss functions in the training script.\n",
    "*   **Image Normalization Statistics (Mean/Std)**: Neural networks train more effectively when input data is normalized (typically to a zero mean and unit variance). This notebook calculates these statistics based on the images as they will be seen by the model (i.e., after resizing). The calculated `mean` and `std` are then used in the `transforms.Normalize` step of the main training pipeline.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "*   `pandas`\n",
    "*   `torch` & `torchvision`\n",
    "*   `Pillow` (PIL)\n",
    "*   `tqdm` (for progress bars)\n",
    "\n",
    "## Usage\n",
    "\n",
    "1.  Modify the `CSV_FILES` and `IMAGE_ROOT_DIRS` variables in the \"USER CONFIGURATION\" cell to point to your data.\n",
    "2.  Ensure the `IMAGE_SIZE` variable matches the input size you plan to use for your neural network.\n",
    "3.  Run all cells of the notebook sequentially.\n",
    "4.  Copy the final `Dataset Mean` and `Dataset Std` values and paste them into the `NORMALIZATION` dictionary in your `config.py` file for the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2178dcbb-7d8f-4857-938c-cbab4626e290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standard Library and Core ML Imports ---\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2f4f13-42f9-4cc1-a803-4a74d34b1bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#                              USER CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. List all your CSV files here\n",
    "CSV_FILES = [\n",
    "    '/path/to/your/metadata.csv',\n",
    "    # Add other CSV files if you have more, e.g.,\n",
    "    # '/path/to/your/second_metadata.csv'\n",
    "]\n",
    "\n",
    "# 2. List all the root directories where your images might be stored\n",
    "IMAGE_ROOT_DIRS = [\n",
    "    '/path/to/your/images',\n",
    "    # Add other root directories if you have them\n",
    "]\n",
    "\n",
    "# 3. Define image processing settings\n",
    "BATCH_SIZE = 1  # How many images to process at once (adjust based on RAM)\n",
    "NUM_WORKERS = 4  # Number of parallel processes for data loading\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4298da2-1015-40e4-9711-d28ba4adbfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part 1: Load and Analyze Categorical Data from CSVs ---\n",
    "\n",
    "print(\"--- Step 1: Loading and Analyzing CSV Metadata ---\")\n",
    "\n",
    "try:\n",
    "    all_dfs = [pd.read_csv(f) for f in CSV_FILES]\n",
    "    df = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"Successfully loaded a total of {len(df)} records from {len(CSV_FILES)} CSV file(s).\\n\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: Could not find a CSV file. Please check the paths in CSV_FILES. Details: {e}\")\n",
    "    raise\n",
    "\n",
    "def print_categorical_stats(dataframe, column_name):\n",
    "    print(f\"--- Statistics for '{column_name}' ---\")\n",
    "    counts = dataframe[column_name].value_counts().sort_index()\n",
    "    percentages = dataframe[column_name].value_counts(normalize=True).sort_index() * 100\n",
    "    stats_df = pd.DataFrame({'Count': counts, 'Percentage (%)': percentages.round(2)})\n",
    "    print(stats_df)\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "columns_to_analyze = ['quadrant', 'tooth', 'root', 'PAI']\n",
    "for col in columns_to_analyze:\n",
    "    if col in df.columns:\n",
    "        print_categorical_stats(df, col)\n",
    "    else:\n",
    "        print(f\"Warning: Column '{col}' not found in the CSV file(s).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9ac04b-a69e-4931-adea-f204420a789a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part 2: Calculate Raw Image Mean and Standard Deviation ---\n",
    "\n",
    "print(\"\\n--- Step 2: Preparing for Raw Image Mean and Standard Deviation Calculation ---\")\n",
    "\n",
    "image_paths = []\n",
    "missing_files = 0\n",
    "for filename in df['filename']:\n",
    "    found_path = None\n",
    "    for root_dir in IMAGE_ROOT_DIRS:\n",
    "        potential_path = os.path.join(root_dir, filename)\n",
    "        if os.path.exists(potential_path):\n",
    "            found_path = potential_path\n",
    "            break\n",
    "    if found_path:\n",
    "        image_paths.append(found_path)\n",
    "    else:\n",
    "        missing_files += 1\n",
    "\n",
    "print(f\"Found {len(image_paths)} image files.\")\n",
    "if missing_files > 0:\n",
    "    print(f\"Warning: {missing_files} image files listed in the CSV were not found.\")\n",
    "\n",
    "class RawImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "stats_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Scales pixels to [0.0, 1.0] and changes to [C, H, W]\n",
    "])\n",
    "\n",
    "# Create the Dataset and DataLoader\n",
    "stats_dataset = RawImageDataset(image_paths, transform=stats_transform)\n",
    "stats_loader = DataLoader(\n",
    "    stats_dataset,\n",
    "    batch_size=BATCH_SIZE, # Must be 1 to handle potentially different image sizes\n",
    "    num_workers=NUM_WORKERS,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "psum = torch.tensor([0.0, 0.0, 0.0])\n",
    "psum_sq = torch.tensor([0.0, 0.0, 0.0])\n",
    "pixel_count = 0\n",
    "\n",
    "print(\"\\nIterating over original-sized images to compute stats...\")\n",
    "for inputs in tqdm(stats_loader):\n",
    "    # Sum over height and width dimensions (dim 2 and 3)\n",
    "    # Since batch_size is 1, dim 0 has size 1.\n",
    "    psum += inputs.sum(dim=[0, 2, 3])\n",
    "    psum_sq += (inputs**2).sum(dim=[0, 2, 3])\n",
    "    \n",
    "    # KEY CHANGE: Accumulate the pixel count dynamically for each image\n",
    "    # C, H, W = inputs.shape[1], inputs.shape[2], inputs.shape[3]\n",
    "    pixel_count += inputs.shape[2] * inputs.shape[3]\n",
    "\n",
    "\n",
    "# Calculate the final mean and std\n",
    "total_mean = psum / pixel_count\n",
    "total_var = (psum_sq / pixel_count) - (total_mean**2)\n",
    "total_std = torch.sqrt(total_var)\n",
    "\n",
    "print(\"\\n--- Raw Image Statistics Calculation Complete ---\")\n",
    "print(f\"Calculated over {len(stats_dataset)} original-sized images.\")\n",
    "print(f\"Dataset Mean (R, G, B): {total_mean.tolist()}\")\n",
    "print(f\"Dataset Std (R, G, B):  {total_std.tolist()}\")\n",
    "print(\"---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b65679-f0d0-4675-a2f7-bf7e58a9a405",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CNN)",
   "language": "python",
   "name": "cnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
