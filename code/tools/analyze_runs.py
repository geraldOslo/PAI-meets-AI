# analyze_runs.py
"""
Utility script for analyzing deep learning training run summaries.

This script scans a specified directory for training summary YAML files (e.g., generated by
`train_utils.py`), extracts key configuration parameters and performance metrics,
and consolidates them into a single, sortable CSV file. This enables easy comparison
and analysis of different experimental runs and hyperparameter tuning results.

It gracefully handles variations in YAML structure and potential missing data,
providing informative warnings.

Author: Gerald Torgersen
SPDX-License-Identifier: MIT
Copyright (c) 2025 Gerald Torgersen, UiO
Contact: https://www.odont.uio.no/iko/english/people/aca/gerald/
"""

import os
import yaml  # Requires pip install pyyaml
import json
import csv
import argparse
import math
import numpy as np
from collections import defaultdict
import re # For regex to convert tuple strings
from datetime import datetime

# --- Define the CSV_HEADERS (Updated for the new YAML/JSON structure) ---
# This list specifies the columns in the output CSV and their order.
CSV_HEADERS = [
    'timestamp',
    'model',  # Changed from model_name to match the YAML key
    'input_size',
    'num_classes',
    'dropout',
    'drop_path_rate',
    'finetune_blocks',
    'epochs',
    'patience',
    'min_delta',
    'base_lr',
    'classifier_lr_multiplier',
    'fast_group_lr_multiplier',
    'weight_decay',
    'grad_clip_max_norm',
    'use_amp',
    'use_oversampler',
    'scheduler_pct_start',
    'scheduler_max_lr_multiplier',
    'warmup_epochs',
    'loss_function_type',
    'label_smoothing',
    'focal_loss_alpha',
    'focal_loss_gamma',
    'class_weights_used',
    'mixup',  # Changed from mixup_enabled
    'mixup_alpha',
    'mixup_prob',
    'batch_size',
    'accum_steps',
    'num_workers',
    'prefetch_factor',
    'pin_memory',
    'persistent_workers',
    'use_horizontal_flip',  # Changed from aug_hflip to match YAML
    'use_rotation',
    'use_color_jitter',
    'use_random_crop',
    'use_centered_affine_scaling',
    'use_affine',
    'use_gamma',
    'use_blur',
    'use_random_erase',
    'rotation_degrees',  # Changed from aug_rotation_degrees
    'brightness_jitter',
    'contrast_jitter',
    'saturation_jitter',
    'hue_jitter',
    'random_crop_scale',
    'random_crop_ratio',
    'translate_range',
    'scale_range',
    'gamma_range',
    'blur_sigma_range',
    'random_erase_probability',
    'random_erase_scale',
    'random_erase_ratio',
    'best_epoch',
    'best_val_accuracy',
    'best_val_loss',
    'best_val_f1',
    'final_train_loss',
    'final_train_accuracy',
    'final_val_loss',
    'final_val_accuracy',
    'final_val_f1',
    'final_val_precision',
    'final_val_recall',
    'device_info',
    'pytorch_version',
    'peak_gpu_memory_gb',
    'yaml_file',
    'json_file',
    'model_file',
    'validation_report_csv',
    'test_metrics_json',
    # --- Test Set Metrics (extracted from test_metrics_json) ---
    'test_accuracy',
    'test_f1_weighted',
    'test_qwk',
    'test_mae',
    'test_binary_accuracy',
    'test_binary_sensitivity',
    'test_binary_specificity',
    'test_binary_fp_rate',
    'test_binary_fn_rate',
    'test_binary_tn',
    'test_binary_fp',
    'test_binary_fn',
    'test_binary_tp',
]

# --- Custom Loader that Ignores Unknown Tags ---
class SafeLoaderIgnoreUnknown(yaml.SafeLoader):
    """Custom YAML loader that ignores unknown Python tags like !!python/tuple"""
    def ignore_unknown_tags(self, node):
        return None

# Register the custom constructor for unknown tags
SafeLoaderIgnoreUnknown.add_constructor(None, SafeLoaderIgnoreUnknown.ignore_unknown_tags)

def parse_tuple_string(s):
    """
    Parse tuple-like strings from YAML (e.g., "(0.85, 1.0)") into Python tuples.
    
    Args:
        s: String, tuple, or list to parse
        
    Returns:
        Tuple of floats or None if parsing fails
    """
    if isinstance(s, (tuple, list)): 
        return tuple(s)
    if isinstance(s, str):
        # Use regex to find numbers within parentheses
        match = re.findall(r"[-+]?\d*\.\d+|\d+", s)
        if match:
            return tuple(float(x) for x in match)
    return None

def parse_yaml_summary(yaml_path):
    """
    Parse YAML summary file with error handling.
    
    Args:
        yaml_path: Path to YAML file
        
    Returns:
        Dictionary with YAML content or None if parsing fails
    """
    try:
        with open(yaml_path, 'r') as f:
            data = yaml.load(f, Loader=SafeLoaderIgnoreUnknown)
        return data
    except FileNotFoundError:
        print(f"  Warning: YAML file not found: {yaml_path}")
        return None
    except yaml.YAMLError as e:
        error_context = ""
        problem_mark = getattr(e, 'problem_mark', None)
        if problem_mark: 
            error_context = f" at line {problem_mark.line+1}, column {problem_mark.column+1}"
        print(f"  Warning: Error parsing YAML file {os.path.basename(yaml_path)}{error_context}: {e}")
        return None
    except Exception as e:
        print(f"  Warning: Unexpected error reading YAML {os.path.basename(yaml_path)}: {e}")
        return None

def parse_json_history(json_path):
    """
    Parse JSON history file with error handling.
    
    Args:
        json_path: Path to JSON file
        
    Returns:
        Dictionary with JSON content or None if parsing fails
    """
    try:
        with open(json_path, 'r') as f:
            data = json.load(f)
        return data
    except FileNotFoundError: 
        return None
    except json.JSONDecodeError as e: 
        print(f"  Warning: Error parsing JSON file {os.path.basename(json_path)}: {e}")
        return None
    except Exception as e: 
        print(f"  Warning: Unexpected error reading JSON {os.path.basename(json_path)}: {e}")
        return None

def extract_run_data(yaml_data, json_data, yaml_filename, json_file_abs_path, config_root_dir):
    """
    Extracts relevant data from parsed YAML and JSON into a flat dictionary.
    Handles both old format (flat structure) and new format (nested structure).
    
    Args:
        yaml_data: Parsed YAML dictionary
        json_data: Parsed JSON dictionary (history)
        yaml_filename: Path to YAML file
        json_file_abs_path: Path to JSON history file
        config_root_dir: Root directory for resolving relative paths
        
    Returns:
        Dictionary with extracted run information or None if extraction fails
    """
    run_info = {}
    if not isinstance(yaml_data, dict):
        print(f"  Error: Invalid YAML data structure for {yaml_filename}. Cannot extract.")
        return None

    # Direct extraction from root level (matching the YAML structure)
    run_info['timestamp'] = yaml_data.get('timestamp')
    run_info['model'] = yaml_data.get('model')
    run_info['input_size'] = yaml_data.get('input_size')
    run_info['num_classes'] = yaml_data.get('num_classes')
    run_info['dropout'] = yaml_data.get('dropout')
    run_info['drop_path_rate'] = yaml_data.get('drop_path_rate')
    run_info['finetune_blocks'] = yaml_data.get('finetune_blocks')
    run_info['epochs'] = yaml_data.get('epochs')
    run_info['patience'] = yaml_data.get('patience')
    run_info['min_delta'] = yaml_data.get('min_delta')
    run_info['base_lr'] = yaml_data.get('base_lr')
    run_info['classifier_lr_multiplier'] = yaml_data.get('classifier_lr_multiplier')
    run_info['fast_group_lr_multiplier'] = yaml_data.get('fast_group_lr_multiplier')
    run_info['weight_decay'] = yaml_data.get('weight_decay')
    run_info['grad_clip_max_norm'] = yaml_data.get('grad_clip_max_norm')
    run_info['use_amp'] = yaml_data.get('use_amp')
    run_info['use_oversampler'] = yaml_data.get('use_oversampler')
    run_info['scheduler_pct_start'] = yaml_data.get('scheduler_pct_start')
    run_info['scheduler_max_lr_multiplier'] = yaml_data.get('scheduler_max_lr_multiplier')
    run_info['warmup_epochs'] = yaml_data.get('warmup_epochs')
    run_info['loss_function_type'] = yaml_data.get('loss_function_type')
    run_info['label_smoothing'] = yaml_data.get('label_smoothing')
    run_info['focal_loss_alpha'] = yaml_data.get('focal_loss_alpha')
    run_info['focal_loss_gamma'] = yaml_data.get('focal_loss_gamma')
    
    # Handle class weights
    class_weights = yaml_data.get('class_weights', [])
    if class_weights and not all(w == 1.0 for w in class_weights):
        run_info['class_weights_used'] = str(class_weights)
    else:
        run_info['class_weights_used'] = None
    
    # Mixup parameters
    run_info['mixup'] = yaml_data.get('mixup')
    run_info['mixup_alpha'] = yaml_data.get('mixup_alpha')
    run_info['mixup_prob'] = yaml_data.get('mixup_prob')
    
    # Batch and dataloader settings
    run_info['batch_size'] = yaml_data.get('batch_size')
    run_info['accum_steps'] = yaml_data.get('accum_steps')
    run_info['num_workers'] = yaml_data.get('num_workers')
    run_info['prefetch_factor'] = yaml_data.get('prefetch_factor')
    run_info['pin_memory'] = yaml_data.get('pin_memory')
    run_info['persistent_workers'] = yaml_data.get('persistent_workers')
    
    # Augmentation flags
    run_info['use_horizontal_flip'] = yaml_data.get('use_horizontal_flip')
    run_info['use_rotation'] = yaml_data.get('use_rotation')
    run_info['use_color_jitter'] = yaml_data.get('use_color_jitter')
    run_info['use_random_crop'] = yaml_data.get('use_random_crop')
    run_info['use_centered_affine_scaling'] = yaml_data.get('use_centered_affine_scaling')
    run_info['use_affine'] = yaml_data.get('use_affine')
    run_info['use_gamma'] = yaml_data.get('use_gamma')
    run_info['use_blur'] = yaml_data.get('use_blur')
    run_info['use_random_erase'] = yaml_data.get('use_random_erase')
    
    # Augmentation parameters
    run_info['rotation_degrees'] = yaml_data.get('rotation_degrees')
    run_info['brightness_jitter'] = yaml_data.get('brightness_jitter')
    run_info['contrast_jitter'] = yaml_data.get('contrast_jitter')
    run_info['saturation_jitter'] = yaml_data.get('saturation_jitter')
    run_info['hue_jitter'] = yaml_data.get('hue_jitter')
    run_info['random_crop_scale'] = yaml_data.get('random_crop_scale')
    run_info['random_crop_ratio'] = yaml_data.get('random_crop_ratio')
    run_info['translate_range'] = yaml_data.get('translate_range')
    run_info['scale_range'] = yaml_data.get('scale_range')
    run_info['gamma_range'] = yaml_data.get('gamma_range')
    run_info['blur_sigma_range'] = yaml_data.get('blur_sigma_range')
    run_info['random_erase_probability'] = yaml_data.get('random_erase_probability')
    run_info['random_erase_scale'] = yaml_data.get('random_erase_scale')
    run_info['random_erase_ratio'] = yaml_data.get('random_erase_ratio')
    
    # Best metrics from best_metrics_achieved section
    best_metrics = yaml_data.get('best_metrics_achieved', {})
    run_info['best_epoch'] = best_metrics.get('best_epoch')
    run_info['best_val_accuracy'] = best_metrics.get('accuracy')
    run_info['best_val_loss'] = best_metrics.get('val_loss')
    run_info['best_val_f1'] = best_metrics.get('f1_score')
    
    # History summary metrics
    history_summary = yaml_data.get('history_summary', {})
    run_info['final_train_loss'] = history_summary.get('train_loss_final')
    run_info['final_train_accuracy'] = history_summary.get('train_acc_final')
    run_info['final_val_loss'] = history_summary.get('val_loss_final')
    run_info['final_val_accuracy'] = history_summary.get('val_acc_final')
    run_info['final_val_f1'] = history_summary.get('f1_final')
    run_info['final_val_precision'] = history_summary.get('precision_final')
    run_info['final_val_recall'] = history_summary.get('recall_final')
    
    # Runtime info
    runtime_info = yaml_data.get('runtime_info', {})
    run_info['device_info'] = runtime_info.get('device')
    run_info['pytorch_version'] = str(runtime_info.get('pytorch_version', ''))
    run_info['peak_gpu_memory_gb'] = runtime_info.get('peak_gpu_memory_gb')
    
    # File paths
    run_info['yaml_file'] = os.path.basename(yaml_filename)
    run_info['json_file'] = os.path.basename(json_file_abs_path) if json_file_abs_path else None
    
    # Extract output file paths
    output_paths = yaml_data.get('output_file_paths', {})
    run_info['model_file'] = os.path.basename(output_paths.get('best_model_checkpoint', ''))
    run_info['validation_report_csv'] = os.path.basename(output_paths.get('validation_report_csv', ''))
    run_info['test_metrics_json'] = os.path.basename(output_paths.get('test_metrics_json', ''))
    
    # Load test metrics from separate JSON file
    test_metrics_json_filename = run_info.get('test_metrics_json')
    if test_metrics_json_filename:
        test_json_path = os.path.join(os.path.dirname(yaml_filename), test_metrics_json_filename)
        test_json_data = parse_json_history(test_json_path)
        
        if test_json_data:
            run_info['test_accuracy'] = test_json_data.get('accuracy')
            run_info['test_f1_weighted'] = test_json_data.get('f1_weighted')
            run_info['test_qwk'] = test_json_data.get('qwk')
            run_info['test_mae'] = test_json_data.get('mae')
            run_info['test_binary_accuracy'] = test_json_data.get('binary_accuracy')
            run_info['test_binary_sensitivity'] = test_json_data.get('binary_sensitivity')
            run_info['test_binary_specificity'] = test_json_data.get('binary_specificity')
            run_info['test_binary_fp_rate'] = test_json_data.get('binary_fp_rate')
            run_info['test_binary_fn_rate'] = test_json_data.get('binary_fn_rate')
            run_info['test_binary_tn'] = test_json_data.get('binary_tn')
            run_info['test_binary_fp'] = test_json_data.get('binary_fp')
            run_info['test_binary_fn'] = test_json_data.get('binary_fn')
            run_info['test_binary_tp'] = test_json_data.get('binary_tp')
    
    # Ensure all header keys exist with None as default
    for key in CSV_HEADERS:
        run_info.setdefault(key, None)
    
    # Clean and format numeric values
    numeric_keys_to_round = [
        'input_size', 'num_classes', 'dropout', 'drop_path_rate', 'finetune_blocks',
        'epochs', 'patience', 'min_delta', 'base_lr', 'classifier_lr_multiplier',
        'fast_group_lr_multiplier', 'weight_decay', 'grad_clip_max_norm',
        'scheduler_pct_start', 'scheduler_max_lr_multiplier', 'warmup_epochs',
        'mixup_alpha', 'mixup_prob', 'batch_size', 'accum_steps', 'num_workers',
        'prefetch_factor', 'rotation_degrees', 'brightness_jitter',
        'contrast_jitter', 'saturation_jitter', 'hue_jitter',
        'random_erase_probability', 'best_epoch', 'best_val_accuracy', 
        'best_val_loss', 'best_val_f1', 'final_train_loss', 'final_train_accuracy', 
        'final_val_loss', 'final_val_accuracy', 'final_val_f1', 'final_val_precision', 
        'final_val_recall', 'peak_gpu_memory_gb', 'test_accuracy', 'test_f1_weighted', 
        'test_qwk', 'test_mae', 'test_binary_accuracy', 'test_binary_sensitivity', 
        'test_binary_specificity', 'test_binary_fp_rate', 'test_binary_fn_rate',
        'test_binary_tn', 'test_binary_fp', 'test_binary_fn', 'test_binary_tp'
    ]
    
    for key in numeric_keys_to_round:
        value = run_info.get(key)
        if value is not None:
            try:
                float_value = float(value)
                if not math.isnan(float_value):
                    # Determine decimal places based on the parameter type
                    if key in ['base_lr', 'weight_decay', 'min_delta']:
                        decimals = 6
                    elif key in ['best_val_f1', 'final_val_f1', 'test_f1_weighted', 'test_qwk', 
                                'test_accuracy', 'test_mae', 'final_val_precision', 'final_val_recall']:
                        decimals = 4
                    elif key in ['best_val_loss', 'final_val_loss', 'final_train_loss']:
                        decimals = 6
                    else:
                        decimals = 2
                    run_info[key] = round(float_value, decimals)
                else:
                    run_info[key] = None
            except (ValueError, TypeError):
                pass
    
    # Handle boolean conversions
    boolean_keys = ['use_amp', 'use_oversampler', 'mixup', 'pin_memory', 'persistent_workers',
                    'use_horizontal_flip', 'use_rotation', 'use_color_jitter', 'use_random_crop',
                    'use_centered_affine_scaling', 'use_affine', 'use_gamma', 'use_blur', 'use_random_erase']
    
    for key in boolean_keys:
        value = run_info.get(key)
        if isinstance(value, str):
            run_info[key] = value.lower() == 'true'
        elif value is None:
            run_info[key] = False
    
    # Parse tuple strings for ranges
    tuple_keys = ['random_crop_scale', 'random_crop_ratio', 'translate_range',
                  'scale_range', 'gamma_range', 'blur_sigma_range',
                  'random_erase_scale', 'random_erase_ratio']
    
    for key in tuple_keys:
        value = run_info.get(key)
        if isinstance(value, str):
            run_info[key] = parse_tuple_string(value)
        elif isinstance(value, (list, tuple)):
            run_info[key] = tuple(round(float(x), 4) for x in value)
    
    # Set default values for sorting keys if None
    if run_info.get('best_val_f1') is None:
        run_info['best_val_f1'] = -1.0
    if run_info.get('test_qwk') is None:
        run_info['test_qwk'] = -1.0
    
    return run_info

def process_directory(input_dir, config_root_dir):
    """
    Scans directory for summary YAML files and extracts training run data.
    
    Args:
        input_dir: Directory containing YAML and JSON files
        config_root_dir: Root directory for resolving relative paths
        
    Returns:
        Tuple of (list of run data dictionaries, count of skipped runs)
    """
    print(f"Scanning directory: {input_dir}")
    all_run_data = []
    processed_bases = set()
    skipped_count = 0

    try:
        all_entries = [entry.name for entry in os.scandir(input_dir) if entry.is_file()]
    except FileNotFoundError:
        print(f"Error: Input directory not found: {input_dir}")
        return [], 0
    except Exception as e:
        print(f"Error listing directory {input_dir}: {e}")
        return [], 0

    # Find all comprehensive summary YAML files
    yaml_files = sorted([f for f in all_entries if f.endswith("_comprehensive_summary.yaml")])
    print(f"Found {len(yaml_files)} summary YAML files.")

    for yaml_filename in yaml_files:
        # Extract base name by removing the suffix
        base_name = yaml_filename[:-len("_comprehensive_summary.yaml")]

        if not base_name or base_name in processed_bases:
            continue

        print(f"Processing run: {base_name}")
        yaml_path = os.path.join(input_dir, yaml_filename)
        json_filename = f"{base_name}_history.json"
        json_path = os.path.join(input_dir, json_filename)

        yaml_data = parse_yaml_summary(yaml_path)
        if yaml_data is None:
            print(f"  Skipping run {base_name} due to YAML parsing error or invalid content.")
            skipped_count += 1
            processed_bases.add(base_name)
            continue

        json_data = parse_json_history(json_path)

        run_info = extract_run_data(yaml_data, json_data, yaml_path, json_path, config_root_dir)
        if run_info:
            all_run_data.append(run_info)
        else:
            print(f"  Skipping run {base_name} due to data extraction error.")
            skipped_count += 1

        processed_bases.add(base_name)

    print(f"Successfully processed {len(all_run_data)} runs.")
    if skipped_count > 0:
        print(f"Skipped {skipped_count} runs due to errors.")
    return all_run_data, skipped_count

def save_to_csv(data_list, output_csv_path):
    """
    Saves extracted run data to CSV file, sorted by test metrics.
    
    Args:
        data_list: List of run data dictionaries
        output_csv_path: Path to output CSV file
    """
    if not data_list:
        print("No data to save.")
        return

    try:
        # Sort by test_qwk (primary), test_f1_weighted (secondary), best_val_f1 (tertiary)
        data_list.sort(key=lambda x: (
            float(x.get('test_qwk', -1.0)) if x.get('test_qwk') is not None else -1.0,
            float(x.get('test_f1_weighted', -1.0)) if x.get('test_f1_weighted') is not None else -1.0,
            float(x.get('best_val_f1', -1.0)) if x.get('best_val_f1') is not None else -1.0
        ), reverse=True)
        print(f"Saving {len(data_list)} runs, sorted by test_qwk (descending).")
    except Exception as sort_e:
        print(f"Warning: Could not sort run data. Saving in processed order. Error: {sort_e}")

    print(f"Saving summary to: {output_csv_path}")
    try:
        with open(output_csv_path, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=CSV_HEADERS, extrasaction='ignore')
            writer.writeheader()
            for run_data in data_list:
                if isinstance(run_data, dict):
                    writer.writerow(run_data)
                else:
                    print(f"  Warning: Skipping invalid run data item: {run_data}")
        print("CSV file saved successfully.")
    except IOError as e:
        print(f"Error writing CSV file {output_csv_path}: {e}")
    except Exception as e:
        print(f"An unexpected error occurred during CSV writing: {e}")

def main():
    """Main function to process training runs and generate summary CSV."""
    parser = argparse.ArgumentParser(description="Analyze training run YAML summaries and create a CSV report.")
    parser.add_argument("input_dir", help="Directory containing the YAML summary and JSON history files.")
    parser.add_argument("-o", "--output", default=None,
                        help="Path to the output CSV file (default: <input_dir>/<timestamp>_training_runs_summary.csv)")
    parser.add_argument("--config_root_dir", default=".",
                        help="Root directory of the project, used to resolve relative paths in YAML.")
    args = parser.parse_args()

    # Generate timestamped filename if not specified
    if args.output is None:
        timestamp = datetime.now().strftime("%Y%m%d-%H%M")
        output_filename = f"{timestamp}_training_runs_summary.csv"
        args.output = os.path.join(args.input_dir, output_filename)
    
    run_data_list, skipped_runs_count = process_directory(args.input_dir, args.config_root_dir)

    if run_data_list:
        save_to_csv(run_data_list, args.output)
    else:
        print("No valid runs processed, CSV file not created.")
    
    # Print summary statistics
    total_files = len(os.listdir(args.input_dir)) if os.path.isdir(args.input_dir) else 0
    print(f"Total files in directory: {total_files}. Processed: {len(run_data_list)}. Skipped: {skipped_runs_count}.")

if __name__ == "__main__":
    main()