{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientNet B3 Training for Periapical Index (PAI) Classification\n",
    "\n",
    "**Author:** Gerald Torgersen  \n",
    "**Date:** 2025  \n",
    "**GitHub:** [github.com/geraldOslo/PAI-meets-AI](https://github.com/geraldOslo/PAI-meets-AI)  \n",
    "\n",
    "**License**  \n",
    "SPDX-License-Identifier: MIT  \n",
    "Copyright (c) 2025 Gerald Torgersen\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook orchestrates the training of a deep learning model (default: EfficientNet-B3) to classify dental radiographs based on the Periapical Index (PAI) scale. The PAI scores (originally 1-5) are mapped to classes 0-4 for training purposes.\n",
    "\n",
    "## Workflow Summary\n",
    "\n",
    "This notebook serves as the main script to run the training process. Core functionalities are modularized into separate Python files (`.py`) for better organization and reusability:\n",
    "\n",
    "-   **`config.py`**: Manages all hyperparameters, data paths, augmentation settings, and other configurations.\n",
    "-   **`data_utils.py`**: Handles dataset loading (`CustomDataset`), data statistics display, and the creation of data augmentation pipelines.\n",
    "-   **`model_utils.py`**: Defines the function (`get_model`) to load pretrained models (like EfficientNet) and adapt them for the PAI classification task (e.g., modify classifier head, apply fine-tuning strategy).\n",
    "-   **`train_utils.py`**: Contains core training helper functions, including loss function definitions (CrossEntropy, FocalLoss), optimizer/scheduler creation (`get_optimizer`, `get_scheduler`), gradient clipping, Mixup (if enabled), checkpointing, and saving results (history JSON, summary YAML).\n",
    "-   **`visualization_utils.py`**: Provides functions to generate and save plots for learning rates, GPU memory usage, and training/validation metrics.\n",
    "\n",
    "The notebook imports these modules and executes the following steps:\n",
    "1.  Load configuration from `config.py`.\n",
    "2.  Load dataset metadata and perform train/validation split.\n",
    "3.  Apply oversampling to the training set.\n",
    "4.  Create PyTorch Datasets and DataLoaders.\n",
    "5.  Initialize the model, loss function, optimizer, and scheduler using utility functions.\n",
    "6.  Run the main training and validation loop.\n",
    "7.  Save the best model checkpoint, training history, plots, and a final summary YAML file.\n",
    "\n",
    "## Configuration\n",
    "\n",
    "All settings are managed externally in `config.py`. Before running this notebook:\n",
    "\n",
    "1.  Ensure `config.py`, `data_utils.py`, `model_utils.py`, `train_utils.py`, and `visualization_utils.py` are in the same directory as this notebook or accessible via Python's path.\n",
    "2.  If starting fresh, copy `config_template.py` (if available) to `config.py`.\n",
    "3.  Edit `config.py` to match your environment (data paths, desired hyperparameters, etc.).\n",
    "\n",
    "## Key Training Techniques Implemented\n",
    "\n",
    "-   Fine-tuning of models pre-trained on ImageNet (e.g., EfficientNet-B3).\n",
    "-   Configurable loss functions (e.g., CrossEntropy with Label Smoothing, Focal Loss).\n",
    "-   Mixed Precision Training (AMP) via `torch.amp` for faster training and reduced memory usage on compatible GPUs.\n",
    "-   AdamW Optimizer.\n",
    "-   OneCycleLR Learning Rate Scheduling for efficient convergence.\n",
    "-   Gradient Clipping to prevent exploding gradients.\n",
    "-   Early Stopping based on validation F1-score to prevent overfitting and save time.\n",
    "-   Optional Mixup data augmentation (configurable in `config.py`).\n",
    "-   Comprehensive logging (training history saved to JSON, run summary saved to YAML) and visualization (metrics, LR, GPU memory plots saved as PNG).\n",
    "\n",
    "## Data Handling\n",
    "\n",
    "-   Loads image paths and PAI labels (originally 1-5) from CSV file(s) specified in `config.py`.\n",
    "-   Maps PAI labels to a 0-based index (0-4) for classification.\n",
    "-   Checks for the existence of image files and filters the dataset accordingly.\n",
    "-   Splits data into training and validation sets using stratified sampling based on PAI class.\n",
    "-   Applies `RandomOverSampler` from `imblearn` to balance the class distribution **only in the training dataset**. The validation set retains its original (potentially imbalanced) distribution for realistic evaluation.\n",
    "-   Applies configurable data augmentation (defined in `config.py` and implemented in `data_utils.py`) to the training set during training.\n",
    "-   Applies standard resizing, center cropping, and normalization to the validation set.\n",
    "-   Displays class distribution statistics for the initial, split, and resampled datasets.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "-   PyTorch (1.8+ recommended)\n",
    "-   torchvision\n",
    "-   scikit-learn\n",
    "-   imblearn (for `RandomOverSampler`)\n",
    "-   pandas\n",
    "-   numpy\n",
    "-   matplotlib\n",
    "-   tqdm\n",
    "-   PyYAML (for saving summary file)\n",
    "-   CUDA-capable GPU (highly recommended for reasonable training times)\n",
    "\n",
    "## Usage\n",
    "\n",
    "1.  Configure `config.py` for your dataset paths and desired hyperparameters.\n",
    "2.  Ensure all required `.py` utility files are present.\n",
    "3.  Run the cells of this notebook sequentially.\n",
    "4.  The best model checkpoint (`_best.pth`), plots, history (`_history.json`), and summary (`_summary.yaml`) will be saved to the directory specified in `config.py` (`CHECKPOINT_DIR`) with a timestamped filename.\n",
    "\n",
    "## Model Application\n",
    "\n",
    "The trained model (`_best.pth` file) can be loaded and used with separate inference scripts to classify new dental radiographs based on the PAI scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import datetime\n",
    "import time\n",
    "import traceback\n",
    "import gc\n",
    "import warnings\n",
    "import sys\n",
    "import copy\n",
    "\n",
    "# Third-party Library Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.amp import GradScaler\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image # Typically used by data_utils for image loading\n",
    "import yaml\n",
    "\n",
    "# Type Hinting Imports\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "# Local Project Utility Imports\n",
    "from config import * # Imports all variables from config.py\n",
    "import data_utils\n",
    "import model_utils\n",
    "import train_utils\n",
    "import visualization_utils\n",
    "\n",
    "# For inline plotting in Jupyter Notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Suppress specific warnings, e.g., from OneCycleLR verbose or AMP anomaly detection\n",
    "warnings.filterwarnings(\"ignore\", message=\"The verbose parameter is deprecated.*\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"Anomaly Detection has been enabled.*\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Setup\n",
    "\n",
    "This section loads parameters from `config.py`, detects the available compute device (CPU/GPU), and sets up all necessary file paths for saving model checkpoints, training history, and plots. The `training_config` dictionary is assembled to consolidate all relevant settings for easy access and later summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- Configuration Loading ---\n",
    "# Data Paths loaded from config.py\n",
    "root_dirs = DATA_PATHS[\"root_dirs\"]\n",
    "csv_files = DATA_PATHS[\"csv_files\"]\n",
    "\n",
    "# Combine relevant configs into a single dictionary for training script use\n",
    "training_config = {}\n",
    "training_config.update(MODEL_CONFIG)\n",
    "training_config.update(NORMALIZATION) # Add normalization info\n",
    "training_config.update(DATALOADER_CONFIG)\n",
    "training_config.update(DATA_TRANSFORM_CONFIG)\n",
    "\n",
    "# --- Runtime Device Detection ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpu_name_detected = \"cpu\"\n",
    "if device.type == 'cuda':\n",
    "    try:\n",
    "        # Get GPU name, sanitize it for filenames\n",
    "        gpu_name_detected = torch.cuda.get_device_name(0).replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not get GPU name: {e}\")\n",
    "        gpu_name_detected = \"gpu_unknown\"\n",
    "print(f\"Runtime Detected Device: {device} ({gpu_name_detected})\")\n",
    "training_config['device_info'] = f\"{device.type} ({gpu_name_detected})\" # Store for summary\n",
    "print(f\"Pytorch version: {torch.__version__}\")\n",
    "\n",
    "model_name = training_config['model']\n",
    "print(f\"Model name: {model_name}\")\n",
    "\n",
    "# --- Timestamp and Save Paths ---\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "training_config['timestamp'] = timestamp  # Store for summary\n",
    "\n",
    "# Create a unique file prefix for all saved assets based on model, device, and timestamp\n",
    "file_prefix = f\"{model_name}_{gpu_name_detected}_{timestamp}\"\n",
    "training_config['file_prefix'] = file_prefix  # Store for summary\n",
    "\n",
    "# Define all output file paths within the CHECKPOINT_DIR\n",
    "best_model_save_path = os.path.join(CHECKPOINT_DIR, f\"{file_prefix}_best.pth\")\n",
    "history_json_path = os.path.join(CHECKPOINT_DIR, f\"{file_prefix}_history.json\")\n",
    "summary_yaml_path = os.path.join(CHECKPOINT_DIR, f\"{file_prefix}_summary.yaml\")\n",
    "gpu_plot_path = os.path.join(CHECKPOINT_DIR, f\"{file_prefix}_gpu_memory.png\")\n",
    "lr_plot_path = os.path.join(CHECKPOINT_DIR, f\"{file_prefix}_lr_schedule.png\")\n",
    "metrics_plot_path = os.path.join(CHECKPOINT_DIR, f\"{file_prefix}_metrics.png\")\n",
    "validation_report_csv_path = os.path.join(CHECKPOINT_DIR, f\"{file_prefix}_validation_report.csv\")\n",
    "\n",
    "# --- Create Checkpoint Directory if it doesn't exist ---\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Display Effective Training Configuration ---\n",
    "print(\"\\n--- Effective Training Configuration ---\")\n",
    "for key, value in training_config.items():\n",
    "     # Optionally shorten long lists/dicts for display to keep console output tidy\n",
    "    if isinstance(value, list) and len(value) > 10:\n",
    "        print(f\"  {key}: [List with {len(value)} items]\")\n",
    "    elif isinstance(value, dict) and len(value) > 5:\n",
    "         print(f\"  {key}: {{Dict with {len(value)} keys}}\")\n",
    "    else:\n",
    "         print(f\"  {key}: {value}\")\n",
    "print(\"-------------------------------------\")\n",
    "print(f\"Best model will be saved to: {best_model_save_path}\")\n",
    "print(f\"Results and logs base path: {CHECKPOINT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation\n",
    "\n",
    "This section handles the loading of dataset metadata from CSV files, performs train-validation splitting with stratification, applies data augmentation transforms, and sets up PyTorch DataLoaders. Oversampling is applied to the training set to address class imbalance, ensuring a more robust training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Loading Data & Creating Datasets using data_utils ---\")\n",
    "\n",
    "# 1. Create Data Transforms using the configuration loaded from config.py\n",
    "print(\"Creating data transforms...\")\n",
    "data_transforms = data_utils.create_data_transforms(DATA_TRANSFORM_CONFIG, NORMALIZATION)\n",
    "print(\"Data transforms creation complete.\")\n",
    "\n",
    "# 2. Load Train/Validation Datasets using the `data_utils.load_datasets` function.\n",
    "# This function handles metadata loading, image file checking, PAI mapping (1-indexed to 0-indexed),\n",
    "# and stratified train/validation splitting.\n",
    "print(\"Loading train/validation datasets...\")\n",
    "train_dataset, val_dataset, _, class_names = data_utils.load_datasets(\n",
    "    data_paths=DATA_PATHS,          # Dictionary of data root directories and CSV files\n",
    "    transforms=data_transforms,     # Dictionary of training and validation transforms\n",
    "    split_test_size=0.2,            # Proportion of data for the validation set\n",
    "    split_random_state=42           # Random seed for reproducible data splitting\n",
    ")\n",
    "\n",
    "# Verify that datasets were successfully loaded\n",
    "if train_dataset is None or val_dataset is None:\n",
    "    raise RuntimeError(\"Failed to load train or validation datasets. Please check data paths and CSV integrity.\")\n",
    "\n",
    "# Store validation set size and class names in the training_config for later summary\n",
    "training_config['val_set_size'] = len(val_dataset)\n",
    "training_config['class_names'] = class_names\n",
    "\n",
    "print(f\"\\nDataset loading and splitting complete via data_utils.\")\n",
    "\n",
    "# --- Create DataLoaders ---\n",
    "# Sets up PyTorch DataLoaders for efficient batch processing during training and validation.\n",
    "# `use_oversampler` flag from config.py controls whether `RandomOverSampler` is applied to the training set.\n",
    "print(\"\\n--- Creating DataLoaders ---\")\n",
    "use_oversampler_flag = training_config.get('use_oversampler', True)\n",
    "print(f\"Creating dataloaders (use_oversampler={use_oversampler_flag})...\")\n",
    "\n",
    "train_loader, val_loader, _ = data_utils.create_dataloaders(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    test_dataset=None,                  # No test loader needed in this training phase\n",
    "    dataloader_config=DATALOADER_CONFIG, # Dataloader specific settings (batch size, workers, etc.)\n",
    "    use_oversampler=use_oversampler_flag\n",
    ")\n",
    "\n",
    "# Verify DataLoaders were created\n",
    "if train_loader is None or val_loader is None:\n",
    "    raise RuntimeError(\"Failed to create train or validation dataloaders.\")\n",
    "\n",
    "# Store dataset sizes in training_config for final summary\n",
    "training_config['original_train_set_size'] = len(train_dataset)\n",
    "try:\n",
    "    # Attempt to get the actual number of samples in the resampled training loader\n",
    "    if hasattr(train_loader, 'sampler') and train_loader.sampler is not None:\n",
    "        if hasattr(train_loader.sampler, 'indices'):\n",
    "            training_config['resampled_train_set_size'] = len(train_loader.sampler.indices)\n",
    "        elif hasattr(train_loader.sampler, 'num_samples'):\n",
    "            training_config['resampled_train_set_size'] = train_loader.sampler.num_samples\n",
    "        else:\n",
    "            training_config['resampled_train_set_size'] = len(train_dataset) # Fallback to original if sampler info is ambiguous\n",
    "    else:\n",
    "        training_config['resampled_train_set_size'] = len(train_loader.dataset) if hasattr(train_loader, 'dataset') else len(train_dataset)\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not determine resampled train set size: {e}\")\n",
    "    training_config['resampled_train_set_size'] = len(train_dataset)\n",
    "\n",
    "print(f\"DataLoaders created.\")\n",
    "print(f\"Original train size: {training_config['original_train_set_size']}\")\n",
    "print(f\"Resampled train size (loader): {training_config['resampled_train_set_size']}\")\n",
    "print(f\"Validation size: {training_config['val_set_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Training Components Initialization\n",
    "\n",
    "This section initializes the deep learning model (e.g., EfficientNet-B3), sets up the appropriate loss function, configures the optimizer, and defines the learning rate scheduler. It also enables NVIDIA's Automatic Mixed Precision (AMP) for faster training on compatible GPUs and cuDNN benchmarking for optimized performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Initializing Training Components ---\")\n",
    "\n",
    "# --- Configuration Variables for Training Loop ---\n",
    "batch_size        = training_config['batch_size']\n",
    "accum_steps       = training_config.get('accum_steps', 1) # Default to 1 if not specified\n",
    "grad_clip_maxnorm = training_config.get('grad_clip_max_norm', None)\n",
    "use_amp           = training_config['use_amp'] and device.type == 'cuda'\n",
    "num_epochs_total  = training_config['epochs']\n",
    "num_classes       = training_config['num_classes']\n",
    "\n",
    "# Determine AMP data type (bfloat16 if supported, else float16)\n",
    "amp_dtype = torch.bfloat16 if use_amp and device.type=='cuda' and torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "# --- Get Model (from model_utils) ---\n",
    "print(\"Building model...\")\n",
    "try:\n",
    "    # Call `get_model` with parameters extracted from `training_config`\n",
    "    # IMPORTANT: Now unpacking two return values from model_utils.get_model\n",
    "    model, fast_backbone_param_names = model_utils.get_model(\n",
    "        model_name=training_config['model'],\n",
    "        num_classes=training_config['num_classes'],\n",
    "        pretrained=True, # Assuming pretrained models are always desired for fine-tuning\n",
    "        dropout_rate=training_config['dropout'],\n",
    "        drop_path_rate=training_config['drop_path_rate'],\n",
    "        finetune_blocks=training_config['finetune_blocks']\n",
    "    )\n",
    "    model = model.to(device) # Move the model to the detected compute device\n",
    "    print(\"Model built and moved to device.\\n\")\n",
    "\n",
    "except KeyError as e:\n",
    "     print(f\"Error: Missing required key '{e}' in training_config for model creation. Check config.py.\\n\")\n",
    "     raise # Re-raise the exception to stop execution\n",
    "except Exception as e:\n",
    "     print(f\"An unexpected error occurred during model creation: {e}\\n\")\n",
    "     traceback.print_exc()\n",
    "     raise\n",
    "\n",
    "\n",
    "# --- Get Criterion (Loss Function) from train_utils ---\n",
    "print(\"Setting up loss function...\")\n",
    "try:\n",
    "    # NOTE: The 'criterion_weights' calculation from previous versions of this cell is now\n",
    "    # handled directly inside train_utils.get_criterion based on the 'use_class_weights'\n",
    "    # and 'class_weights' parameters in 'training_config'.\n",
    "    \n",
    "    # Initialize the loss function using train_utils.get_criterion\n",
    "    criterion = train_utils.get_criterion(\n",
    "        config=training_config,\n",
    "        device=device\n",
    "    )\n",
    "    print(f\"Loss function '{training_config['loss_function_type']}' set up.\\n\")\n",
    "except Exception as e:\n",
    "     print(f\"Error setting up loss function: {e}\\n\")\n",
    "     traceback.print_exc()\n",
    "     raise\n",
    "\n",
    "\n",
    "# --- Get Optimizer and Scheduler from train_utils ---\n",
    "print(\"Setting up optimizer and scheduler...\\n\")\n",
    "try:\n",
    "    if 'train_loader' not in locals() or train_loader is None:\n",
    "         raise NameError(\"train_loader is not defined. Ensure data loading cell was run successfully.\")\n",
    "\n",
    "    # Calculate steps per epoch for OneCycleLR scheduler based on training data size and accumulation steps\n",
    "    steps_per_epoch = math.ceil(len(train_loader) / accum_steps)\n",
    "\n",
    "    # Initialize optimizer using train_utils.get_optimizer\n",
    "    # IMPORTANT: Passing the dynamically generated fast_backbone_param_names\n",
    "    optimizer = train_utils.get_optimizer(\n",
    "        model=model,\n",
    "        config=training_config,\n",
    "        fast_backbone_param_names=fast_backbone_param_names # <-- PASS THIS LIST\n",
    "    )\n",
    "\n",
    "    # Initialize scheduler using train_utils.get_scheduler\n",
    "    scheduler = train_utils.get_scheduler(\n",
    "        optimizer=optimizer,\n",
    "        config=training_config,\n",
    "        steps_per_epoch=steps_per_epoch\n",
    "    )\n",
    "\n",
    "    # Store optimizer and scheduler types in training_config for summary\n",
    "    training_config['optimizer_type'] = type(optimizer).__name__\n",
    "    training_config['scheduler_type'] = type(scheduler).__name__ if scheduler else 'None'\n",
    "    print(f\"Optimizer '{training_config['optimizer_type']}' and Scheduler '{training_config['scheduler_type']}' set up.\\n\")\n",
    "except NameError as ne:\n",
    "     print(f\"Error: {ne}\\n\")\n",
    "     raise\n",
    "except Exception as e:\n",
    "     print(f\"An unexpected error occurred setting up optimizer/scheduler: {e}\\n\")\n",
    "     traceback.print_exc()\n",
    "     raise\n",
    "\n",
    "\n",
    "# --- Enable cuDNN benchmark (optional, for CUDA-enabled GPUs) ---\n",
    "if device.type == 'cuda':\n",
    "    print(\"\\nEnabling cuDNN benchmark for optimized CUDA performance.\\n\")\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(\"\\n--- Initialization of Training Components Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Loop\n",
    "\n",
    "This is the core training loop, iterating over a specified number of epochs. It includes:\n",
    "-   Gradient accumulation for larger effective batch sizes.\n",
    "-   Automatic Mixed Precision (AMP) for performance.\n",
    "-   Gradient clipping to prevent exploding gradients.\n",
    "-   Real-time monitoring of training and validation metrics (loss, accuracy, F1-score).\n",
    "-   Early stopping mechanism based on validation F1-score to prevent overfitting.\n",
    "-   Logging of learning rate and GPU memory usage.\n",
    "-   Checkpointing the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- Debugging Flag ---\n",
    "# Set to True to enable anomaly detection (slower, for debugging NaNs/inf), False for normal runs.\n",
    "debug_anomaly = False\n",
    "\n",
    "# --- ANSI Color Codes for enhanced console output ---\n",
    "COLOR_GREEN = '\\033[92m'\n",
    "COLOR_YELLOW = '\\033[93m'\n",
    "COLOR_RED = '\\033[91m'\n",
    "COLOR_RESET = '\\033[0m'\n",
    "\n",
    "# --- Verify and Extract Training Configuration Parameters ---\n",
    "num_epochs_total = training_config.get('epochs', 90)\n",
    "accum_steps = training_config.get('accum_steps', 1)\n",
    "use_amp = device.type == 'cuda' and training_config.get('use_amp', True)\n",
    "min_delta = training_config.get('min_delta', 0.001)\n",
    "patience  = training_config.get('patience', 20)\n",
    "grad_clip_maxnorm = training_config.get('grad_clip_max_norm', None)\n",
    "num_classes = training_config.get('num_classes', 5)\n",
    "batch_size = training_config.get('batch_size', 64)\n",
    "\n",
    "# Create model configuration dictionary for checkpoint saving\n",
    "model_config_for_checkpoint = {\n",
    "    'model_name': training_config.get('model', 'unknown'),\n",
    "    'dropout_rate': training_config.get('dropout', 0.0),\n",
    "    'drop_path_rate': training_config.get('drop_path_rate', 0.0),\n",
    "    'finetune_blocks': training_config.get('finetune_blocks', 0),\n",
    "    'num_classes': training_config.get('num_classes', 5)\n",
    "}\n",
    "\n",
    "# Determine AMP data type (bfloat16 if supported, else float16)\n",
    "amp_dtype = torch.bfloat16 if use_amp and device.type=='cuda' and torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"▶️ Training for {num_epochs_total} epochs -- Accumulation Steps: {accum_steps}\")\n",
    "print(f\"   Optimizer:  {training_config.get('optimizer_type', 'AdamW')}\")\n",
    "print(f\"   Scheduler:  {training_config.get('scheduler_type', 'OneCycleLR')}\")\n",
    "print(f\"   Device:     {device}  |  AMP Enabled: {use_amp} (dtype: {amp_dtype})\")\n",
    "print(f\"   Batch size: {batch_size}   |  Effective batch size: {batch_size * accum_steps}\")\n",
    "if debug_anomaly:\n",
    "    print(f\"   {COLOR_YELLOW}Anomaly Detection: ENABLED (Slower execution){COLOR_RESET}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# --- Initialize Training State Variables ---\n",
    "# `best_metrics_summary` tracks the best validation performance achieved so far.\n",
    "best_metrics_summary = {'f1': -1.0, 'acc': 0.0, 'loss': float('inf'), 'epoch': -1}\n",
    "\n",
    "# Lists to store predictions, labels, and filenames from the best validation epoch.\n",
    "best_epoch_val_preds_final = []\n",
    "best_epoch_val_labels_final = []\n",
    "best_epoch_val_filenames_final = []\n",
    "\n",
    "# `history` dictionary to log metrics for each epoch.\n",
    "history = {k: [] for k in ['train_loss','train_acc','val_loss','val_acc', 'precision','recall','f1','lr','gpu_mem_used']}\n",
    "\n",
    "pat_ctr = 0 # Patience counter for early stopping\n",
    "start_epoch = 0 # Start epoch for the training loop (can be adjusted for resuming training)\n",
    "\n",
    "# Ensure required components are initialized from previous cells\n",
    "if 'model' not in locals() or 'criterion' not in locals() or 'optimizer' not in locals():\n",
    "     raise NameError(\"Model, criterion, or optimizer objects were not initialized. Please run previous cells.\")\n",
    "scheduler = locals().get('scheduler', None) # Scheduler might be None if not configured\n",
    "\n",
    "# Initialize GradScaler for Automatic Mixed Precision (AMP)\n",
    "scaler = torch.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "# Print header for the training progress table\n",
    "print(\"\\n\" + \"=\"*101)\n",
    "print(f\"| {'Epoch':<7} | {'Train Loss':<10} | {'Train Acc':<9} | \"\n",
    "      f\"{'Val Loss':<8} | {'Val Acc':<7} | {'Precision':<9} | \"\n",
    "      f\"{'Recall':<6} | {'F1-Score':<8} | {'GPU Mem':<9} |\")\n",
    "print(\"|\" + \"-\"*99 + \"|\")\n",
    "\n",
    "try:\n",
    "    for epoch in range(start_epoch, num_epochs_total):\n",
    "\n",
    "        # ------------------ TRAINING PHASE ------------------\n",
    "        model.train() # Set model to training mode\n",
    "        run_loss_epoch = 0.0\n",
    "        preds_ep, labs_ep = [], []\n",
    "        skipped_batches_train = 0\n",
    "\n",
    "        # Zero gradients at the beginning of an accumulation cycle if accumulation is enabled\n",
    "        if accum_steps <= 1: optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs_total} [Train]\",\n",
    "                          leave=False, unit=\"batch\", dynamic_ncols=True, ascii=True, file=sys.stdout)\n",
    "\n",
    "        for b, batch_data in enumerate(train_pbar):\n",
    "            # Gradient Accumulation: Zero gradients if starting a new accumulation cycle\n",
    "            if b % accum_steps == 0 and accum_steps > 1:\n",
    "                 optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Load inputs and labels to device\n",
    "            try: \n",
    "                inputs, labels, _ = batch_data\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True, dtype=torch.long)\n",
    "            except Exception as e:\n",
    "                print(f\"\\n{COLOR_YELLOW}Warn: Skipping train batch {b} due to data loading error: {e}{COLOR_RESET}\")\n",
    "                skipped_batches_train+=1\n",
    "                continue\n",
    "\n",
    "            # Define a nested function for the forward and backward pass, optionally with anomaly detection\n",
    "            def forward_backward_step():\n",
    "                with torch.amp.autocast(device_type=device.type, enabled=use_amp, dtype=amp_dtype):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels) / accum_steps # Scale loss by accumulation steps\n",
    "                if not torch.isfinite(loss):\n",
    "                    print(f\"\\n{COLOR_YELLOW}Warn: Non-finite loss ({loss.item()*accum_steps:.4f}) in train batch {b} epoch {epoch+1}. Skipping this batch.{COLOR_RESET}\")\n",
    "                    return False, None, None\n",
    "                scaler.scale(loss).backward() # Scale loss and perform backward pass\n",
    "                return True, outputs, loss\n",
    "\n",
    "            step_successful = False\n",
    "            outer_exception = None\n",
    "            step_outputs, step_loss = None, None\n",
    "\n",
    "            # Execute forward/backward step with optional anomaly detection\n",
    "            if debug_anomaly:\n",
    "                try:\n",
    "                    with torch.autograd.detect_anomaly(check_nan=True): \n",
    "                        step_successful, step_outputs, step_loss = forward_backward_step()\n",
    "                except RuntimeError as e: \n",
    "                    outer_exception = e; step_successful=False\n",
    "                except Exception as e: \n",
    "                    outer_exception = e; step_successful=False\n",
    "            else:\n",
    "                 try: \n",
    "                     step_successful, step_outputs, step_loss = forward_backward_step()\n",
    "                 except RuntimeError as e: \n",
    "                     outer_exception = e; step_successful=False\n",
    "                 except Exception as e: \n",
    "                     outer_exception = e; step_successful=False\n",
    "\n",
    "            # Handle unsuccessful steps (e.g., non-finite loss, runtime errors)\n",
    "            if outer_exception is not None or not step_successful:\n",
    "                if outer_exception: print(f\"\\n{COLOR_RED}Error during train step (batch {b}, Epoch {epoch+1}): {outer_exception}{COLOR_RESET}\")\n",
    "                skipped_batches_train += 1\n",
    "                # Clear gradients if an error occurred before the optimizer step for this accumulation cycle\n",
    "                if (b + 1) % accum_steps != 0: optimizer.zero_grad(set_to_none=True)\n",
    "                continue # Skip to the next batch\n",
    "\n",
    "            # Accumulate loss and predictions from successful steps\n",
    "            unscaled_loss_val = step_loss.item() * accum_steps\n",
    "            run_loss_epoch += unscaled_loss_val * inputs.size(0) # Total accumulated loss for the epoch\n",
    "            if step_outputs is not None:\n",
    "                 with torch.no_grad(): # Ensure argmax operation doesn't build a computation graph\n",
    "                     preds_ep.extend(step_outputs.argmax(1).cpu().numpy())\n",
    "                     labs_ep.extend(labels.cpu().numpy())\n",
    "            train_pbar.set_postfix(loss=f\"{unscaled_loss_val:.4f}\")\n",
    "\n",
    "            # --- Gradient Accumulation: Perform optimizer step if accumulation cycle is complete ---\n",
    "            if (b + 1) % accum_steps == 0:\n",
    "                try:\n",
    "                    scaler.unscale_(optimizer) # Unscale gradients before clipping\n",
    "                    if grad_clip_maxnorm and grad_clip_maxnorm > 0: \n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_maxnorm)\n",
    "                    scaler.step(optimizer) # Perform optimizer step\n",
    "                    scaler.update() # Update the scale for the next iteration\n",
    "                    optimizer.zero_grad(set_to_none=True) # Zero gradients after the optimizer step\n",
    "                    # Step LR scheduler (typically per batch for OneCycleLR)\n",
    "                    if scheduler and isinstance(scheduler, torch.optim.lr_scheduler.OneCycleLR): \n",
    "                        scheduler.step()\n",
    "                except Exception as step_e:\n",
    "                     print(f\"\\n{COLOR_RED}Error during optimizer/scheduler step (batch {b}, Epoch {epoch+1}): {step_e}{COLOR_RESET}\")\n",
    "                     optimizer.zero_grad(set_to_none=True) # Ensure gradients are cleared even on error\n",
    "\n",
    "        # Handle any remaining gradients if the last accumulation cycle is incomplete\n",
    "        if accum_steps > 1 and (b + 1) % accum_steps != 0:\n",
    "            try:\n",
    "                print(f\"  (Performing final optimizer step for epoch {epoch+1} with partial batch)\")\n",
    "                scaler.unscale_(optimizer)\n",
    "                if grad_clip_maxnorm and grad_clip_maxnorm > 0: \n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_maxnorm)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "            except Exception as final_step_e:\n",
    "                 print(f\"\\n{COLOR_RED}Error during final optimizer step (Epoch {epoch+1}): {final_step_e}{COLOR_RESET}\")\n",
    "\n",
    "        # Calculate average training loss and accuracy for the epoch\n",
    "        num_train_samples_processed = len(labs_ep)\n",
    "        if num_train_samples_processed > 0:\n",
    "            tr_loss = run_loss_epoch / num_train_samples_processed\n",
    "            tr_acc = accuracy_score(labs_ep, preds_ep)\n",
    "        else: \n",
    "            tr_loss = float('nan')\n",
    "            tr_acc = 0.0\n",
    "            print(f\"{COLOR_YELLOW}Warning: No valid training samples processed in epoch {epoch+1}. Metrics for this epoch will be zero/NaN.{COLOR_RESET}\")\n",
    "\n",
    "        # Log training metrics and current learning rate\n",
    "        history['train_loss'].append(tr_loss)\n",
    "        history['train_acc'].append(tr_acc)\n",
    "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        # ------------------ VALIDATION PHASE ------------------\n",
    "        model.eval() # Set model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        v_preds, v_labs, v_files = [], [], []\n",
    "        skipped_batches_val = 0\n",
    "\n",
    "        with torch.no_grad(): # Disable gradient calculations for validation\n",
    "            val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs_total} [Val]  \", \n",
    "                            leave=False, unit=\"batch\", dynamic_ncols=True, ascii=True, file=sys.stdout)\n",
    "            for x, y, fn in val_pbar:\n",
    "                 try:\n",
    "                     x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True, dtype=torch.long)\n",
    "                 except Exception as e:\n",
    "                     print(f\"\\n{COLOR_YELLOW}Warn: Skipping validation batch due to data loading error: {e}{COLOR_RESET}\")\n",
    "                     skipped_batches_val+=1\n",
    "                     continue\n",
    "                 try:\n",
    "                    with torch.amp.autocast(device_type=device.type, enabled=use_amp, dtype=amp_dtype):\n",
    "                        outputs = model(x)\n",
    "                        if not torch.all(torch.isfinite(outputs)):\n",
    "                            print(f\"\\n{COLOR_YELLOW}Warn: Non-finite outputs in validation epoch {epoch+1}. Skipping this batch.{COLOR_RESET}\"); skipped_batches_val+=1; continue\n",
    "                        loss = criterion(outputs, y)\n",
    "                    loss_value = loss.item()\n",
    "                    if not math.isfinite(loss_value):\n",
    "                        print(f\"\\n{COLOR_YELLOW}Warn: Non-finite validation loss in epoch {epoch+1}. Skipping this batch.{COLOR_RESET}\"); skipped_batches_val+=1; continue\n",
    "                    \n",
    "                    val_loss += loss_value * x.size(0) # Accumulate validation loss\n",
    "                    v_preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "                    v_labs.extend(y.cpu().numpy())\n",
    "                    v_files.extend(fn if isinstance(fn,(list,tuple)) else [fn])\n",
    "                 except Exception as val_batch_e:\n",
    "                     print(f\"Error processing validation batch: {val_batch_e}\")\n",
    "                     skipped_batches_val+=1\n",
    "                     continue\n",
    "\n",
    "        # Calculate validation metrics\n",
    "        num_val_samples_processed = len(v_labs)\n",
    "        if num_val_samples_processed > 0:\n",
    "            val_loss /= num_val_samples_processed # Average loss per sample\n",
    "            val_acc = accuracy_score(v_labs, v_preds)\n",
    "            # Calculate precision, recall, F1-score (weighted average for multi-class)\n",
    "            prec, rec, f1, _ = precision_recall_fscore_support(v_labs, v_preds, average='weighted', zero_division=0, labels=list(range(num_classes)))\n",
    "        else:\n",
    "            val_loss, val_acc, prec, rec, f1 = float('nan'), 0.0, 0.0, 0.0, 0.0\n",
    "            print(f\"{COLOR_YELLOW}Warning: No valid validation samples processed in epoch {epoch+1}. Metrics for this epoch will be zero/NaN.{COLOR_RESET}\")\n",
    "\n",
    "        # Log validation metrics\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc*100.0) # Store as percentage\n",
    "        history['precision'].append(prec)\n",
    "        history['recall'].append(rec)\n",
    "        history['f1'].append(f1)\n",
    "\n",
    "        # --- Get GPU Memory Usage ---\n",
    "        mem_used_gb = 0.0\n",
    "        if device.type == 'cuda':\n",
    "            try:\n",
    "                # Use memory_reserved for a better estimate of total GPU footprint including cached memory\n",
    "                mem_bytes = torch.cuda.memory_reserved(device)\n",
    "                mem_used_gb = mem_bytes / (1024**3) # Convert bytes to Gigabytes\n",
    "            except Exception:\n",
    "                mem_used_gb = 0.0 # Default to 0 on error, without printing a warning every epoch\n",
    "        history['gpu_mem_used'].append(mem_used_gb)\n",
    "\n",
    "        # Print epoch summary in a formatted table row\n",
    "        print(f\"| {epoch+1:<7} | {tr_loss:<10.6f} | {tr_acc*100:<9.2f}% | {val_loss:<8.6f} | {val_acc*100:<7.2f}% | {prec:<9.4f} | {rec:<6.4f} | {f1:<8.6f} | {mem_used_gb:<9.2f} GB |\")\n",
    "\n",
    "        # Step epoch-based schedulers (e.g., ReduceLROnPlateau) if OneCycleLR is not used\n",
    "        if scheduler and not isinstance(scheduler, torch.optim.lr_scheduler.OneCycleLR):\n",
    "             if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau): \n",
    "                 scheduler.step(f1) # Step with validation F1-score\n",
    "             else: \n",
    "                 scheduler.step() # Step without a specific metric\n",
    "\n",
    "        # --------- Early Stopping Logic and Best Model Checkpointing ----------\n",
    "        current_f1 = f1 if isinstance(f1, float) and math.isfinite(f1) else 0.0\n",
    "        # Check for significant F1 improvement (current_f1 must be > best_f1 + min_delta)\n",
    "        f1_improved = current_f1 > best_metrics_summary['f1'] + min_delta\n",
    "\n",
    "        if f1_improved:\n",
    "            improvement_msg = f\"{COLOR_GREEN}✅ F1 improved from {best_metrics_summary['f1']:.4f} to {current_f1:.4f}. Saving model...{COLOR_RESET}\"\n",
    "            print(improvement_msg)\n",
    "            # Update best metrics summary\n",
    "            best_metrics_summary.update(f1=current_f1, acc=val_acc*100.0, loss=val_loss, epoch=epoch+1)\n",
    "            pat_ctr = 0 # Reset patience counter on improvement\n",
    "            try:\n",
    "                # Save the model checkpoint using `train_utils.save_checkpoint`\n",
    "                # Save the model checkpoint using `train_utils.save_checkpoint`\n",
    "                train_utils.save_checkpoint(\n",
    "                    model=model,\n",
    "                    optimizer=optimizer,\n",
    "                    epoch=epoch,\n",
    "                    best_metric_val=current_f1,\n",
    "                    path=best_model_save_path,\n",
    "                    model_config=model_config_for_checkpoint  # Add this parameter\n",
    "                )\n",
    "                \n",
    "                # Store validation results from this best epoch for the final report\n",
    "                best_epoch_val_preds_final = v_preds[:] \n",
    "                best_epoch_val_labels_final = v_labs[:] \n",
    "                best_epoch_val_filenames_final = v_files[:]\n",
    "                \n",
    "            except Exception as save_e: \n",
    "                print(f\"  {COLOR_RED}❌ Error saving best model checkpoint: {save_e}{COLOR_RESET}\")\n",
    "                traceback.print_exc()\n",
    "        else:\n",
    "            pat_ctr += 1 # Increment patience counter if no improvement\n",
    "            warning_msg = f\"{COLOR_YELLOW}⚠️ No F1 improvement for {pat_ctr}/{patience} epochs. (Best F1: {best_metrics_summary['f1']:.4f}){COLOR_RESET}\"\n",
    "            print(warning_msg)\n",
    "            if not (isinstance(f1, float) and math.isfinite(f1)): \n",
    "                print(f\"   {COLOR_YELLOW}(Current epoch F1 score was NaN or invalid, treated as no improvement){COLOR_RESET}\")\n",
    "\n",
    "        # Check if early stopping criteria are met\n",
    "        if pat_ctr >= patience:\n",
    "            print(f\"\\n{COLOR_RED}🛑 Early stopping triggered. Patience limit ({patience} epochs) reached.{COLOR_RESET}\")\n",
    "            training_config['patience_counter'] = pat_ctr # Store the final patience counter value\n",
    "            # If no improvement was ever recorded, use the last epoch's validation results for reporting\n",
    "            if not best_epoch_val_labels_final and v_labs:\n",
    "                print(f\"{COLOR_YELLOW}   (Using validation results from last epoch {epoch+1} for final report as no prior improvement was recorded).{COLOR_RESET}\")\n",
    "                best_epoch_val_preds_final = v_preds[:]\n",
    "                best_epoch_val_labels_final = v_labs[:]\n",
    "                best_epoch_val_filenames_final = v_files[:]\n",
    "            break # Exit the training loop\n",
    "\n",
    "except RuntimeError as e:\n",
    "    print(f\"\\n{COLOR_RED}--- Training Loop Runtime Error ---{COLOR_RESET}\\n{COLOR_RED}{e}{COLOR_RESET}\")\n",
    "    traceback.print_exc()\n",
    "except KeyboardInterrupt:\n",
    "    print(f\"\\n{COLOR_YELLOW}--- Training Interrupted by User ---{COLOR_RESET}\")\n",
    "finally:\n",
    "    # Assign final results for the next cells using the tracked best epoch data\n",
    "    epochs_completed = epoch + 1 # +1 because epoch is 0-indexed\n",
    "    training_config['epochs_completed'] = epochs_completed\n",
    "    \n",
    "    best_epoch_val_labels = best_epoch_val_labels_final\n",
    "    best_epoch_val_preds = best_epoch_val_preds_final\n",
    "    best_epoch_val_filenames = best_epoch_val_filenames_final\n",
    "    best_f1 = best_metrics_summary['f1']\n",
    "    best_acc = best_metrics_summary['acc']\n",
    "    best_val_loss = best_metrics_summary['loss']\n",
    "\n",
    "    # Clean up GPU memory and Python objects\n",
    "    try:\n",
    "        del model, criterion, optimizer, scheduler, train_loader, val_loader, train_dataset, val_dataset\n",
    "        del inputs, labels, outputs, loss # Attempt to delete tensors if they still exist\n",
    "    except NameError: # Catch if some variables were not defined due to early exit\n",
    "        pass \n",
    "    except UnboundLocalError: # Catch if some variables were not defined due to early exit\n",
    "        pass\n",
    "    gc.collect() # Force garbage collection\n",
    "    if device.type == 'cuda': \n",
    "        torch.cuda.empty_cache() # Clear CUDA cache\n",
    "\n",
    "    print(\"\\n\" + \"-\"*99 + \"|\")\n",
    "    print(\"🏁 Training loop finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Training Analysis and Saving Results\n",
    "\n",
    "After the training loop concludes (either by reaching maximum epochs or early stopping), this section handles the saving of comprehensive training history, logs, and plots. This ensures that the results are fully reproducible and accessible for later analysis and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"📊 Post-Training: Saving Training History and Plots\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# `output_files_for_summary` dictionary compiles all paths for easy reference in the final summary.\n",
    "best_f1_achieved = best_metrics_summary.get('f1', -1.0)\n",
    "output_files_for_summary = {\n",
    "    'best_model_path': best_model_save_path if best_f1_achieved > -1.0 and os.path.exists(best_model_save_path) else None,\n",
    "    'history_json_path': history_json_path,\n",
    "    'metrics_plot_path': metrics_plot_path,\n",
    "    'lr_plot_path': lr_plot_path,\n",
    "    'gpu_plot_path': gpu_plot_path,\n",
    "    'summary_yaml_path': summary_yaml_path,\n",
    "    'validation_report_csv': validation_report_csv_path,\n",
    "}\n",
    "print(f\"  Output file paths prepared.\")\n",
    "\n",
    "# --- Save Training History to JSON ---\n",
    "print(f\"\\nAttempting to save training history to: {history_json_path}\")\n",
    "if history and history_json_path: \n",
    "    try:\n",
    "        # Convert numpy types within the history dictionary for JSON serialization compatibility\n",
    "        serializable_history = {}\n",
    "        for key, value in history.items():\n",
    "            if isinstance(value, list) and len(value) > 0 and isinstance(value[0], np.generic):\n",
    "                 serializable_history[key] = [item.item() for item in value]\n",
    "            elif isinstance(value, np.ndarray):\n",
    "                 serializable_history[key] = value.tolist()\n",
    "            else:\n",
    "                 serializable_history[key] = value\n",
    "        with open(history_json_path, 'w') as f:\n",
    "            json.dump(serializable_history, f, indent=2)\n",
    "        print(f\"  History saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error saving history to JSON: {e}\")\n",
    "else:\n",
    "    print(\"  Skipping history saving (history empty or path not defined).\")\n",
    "\n",
    "# --- Generate and Save Plots using `visualization_utils` ---\n",
    "print(\"\\nGenerating and saving plots...\")\n",
    "try:\n",
    "    # Use the correct function names with _and_save suffix\n",
    "    visualization_utils.plot_metrics_and_save(history, metrics_plot_path, model_name=training_config.get('model', 'Model'))\n",
    "    visualization_utils.plot_lr_schedule_and_save(history, lr_plot_path)\n",
    "    visualization_utils.plot_gpu_memory_and_save(history, gpu_plot_path)\n",
    "    print(\"  Plots generated and saved successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error generating or saving plots: {e}\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "# --- Save Validation Report CSV ---\n",
    "print(f\"\\nAttempting to save detailed validation report to: {validation_report_csv_path}\")\n",
    "if best_epoch_val_labels and best_epoch_val_preds and validation_report_csv_path:\n",
    "    try:\n",
    "        # Remove class_names parameter - it's not in the function signature\n",
    "        train_utils.save_validation_report(\n",
    "            labels=best_epoch_val_labels, \n",
    "            predictions=best_epoch_val_preds,\n",
    "            filenames=best_epoch_val_filenames,\n",
    "            num_classes=num_classes,  # This parameter exists\n",
    "            output_path=validation_report_csv_path\n",
    "        )\n",
    "        print(\"  Detailed validation report saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error saving validation report: {e}\")\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"  Skipping detailed validation report saving (no validation data or path not defined).\")\n",
    "\n",
    "print(\"\\n--- Results Saving Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set Evaluation (Optional)\n",
    "\n",
    "This section performs a final evaluation of the best-trained model on a separate, unseen test dataset (if configured). This provides an unbiased assessment of the model's generalization performance. Metrics calculated on the test set are saved to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"➡️ Starting Final Test Set Inference (Optional)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# --- Configuration and Paths for Test Data ---\n",
    "test_data_configured = 'test_root_dir' in DATA_PATHS and 'test_csv_file' in DATA_PATHS\n",
    "test_root_dir = DATA_PATHS.get('test_root_dir')\n",
    "test_csv_file = DATA_PATHS.get('test_csv_file')\n",
    "best_model_path_to_load = best_model_save_path # Path to the best model saved during training\n",
    "\n",
    "\n",
    "\n",
    "# --- Check Prerequisites for Test Inference ---\n",
    "best_model_exists = best_model_path_to_load and os.path.exists(best_model_path_to_load)\n",
    "test_csv_exists = test_csv_file and os.path.exists(test_csv_file)\n",
    "test_dir_exists = test_root_dir and os.path.isdir(test_root_dir)\n",
    "\n",
    "print(f\"Test Data Configured in config.py: {test_data_configured}\")\n",
    "print(f\"Test Root Directory Exists: {test_dir_exists} ({test_root_dir})\")\n",
    "print(f\"Test CSV File Exists: {test_csv_exists} ({test_csv_file})\")\n",
    "print(f\"Best Model Checkpoint Exists: {best_model_exists} ({best_model_path_to_load})\")\n",
    "\n",
    "if test_data_configured and best_model_exists and test_csv_exists and test_dir_exists:\n",
    "    print(\"\\nAll prerequisites met for test set inference. Proceeding...\")\n",
    "    try:\n",
    "        # --- Prepare Test Transforms (using validation transforms logic) ---\n",
    "        print(f\"Preparing test data transforms...\")\n",
    "        test_transforms_dict = data_utils.create_data_transforms(\n",
    "             DATA_TRANSFORM_CONFIG, NORMALIZATION, create_train=False, create_val=True # Use validation transforms for test\n",
    "        )\n",
    "        test_transform = test_transforms_dict['val']\n",
    "\n",
    "        # --- Load Test Dataset ---\n",
    "        print(f\"Loading test dataset from: {test_csv_file}...\")\n",
    "        _, _, test_dataset_final, test_class_names = data_utils.load_datasets(\n",
    "            data_paths=DATA_PATHS,\n",
    "            transforms={'val': test_transform},\n",
    "            load_split='test'\n",
    "        )\n",
    "\n",
    "        final_class_names = training_config.get('class_names', test_class_names)\n",
    "        num_classes = training_config.get('num_classes', len(final_class_names) if final_class_names else 0)\n",
    "\n",
    "        if test_dataset_final and num_classes > 0:\n",
    "            print(f\"Test dataset loaded with {len(test_dataset_final)} samples.\")\n",
    "            # --- Create Test DataLoader ---\n",
    "            print(\"Creating test dataloader...\")\n",
    "            # Use a potentially larger batch size for inference as gradients are not computed\n",
    "            test_loader_config = DATALOADER_CONFIG.copy()\n",
    "            # Set test_batch_size, or use existing batch_size * 2 for validation, then adjust\n",
    "            # to prevent exceeding GPU memory for large models or high-res inputs.\n",
    "            # The original code attempts to load at 256*2=512, which can be too large.\n",
    "            # Let's use a more conservative default from DATALOADER_CONFIG itself for test\n",
    "            test_loader_config['batch_size'] = test_loader_config.get('test_batch_size', test_loader_config.get('batch_size', 64) * 2)\n",
    "\n",
    "            test_loader_config['shuffle'] = False # No need to shuffle for inference\n",
    "\n",
    "            _, _, test_loader_final = data_utils.create_dataloaders(\n",
    "                train_dataset=None,\n",
    "                val_dataset=None,\n",
    "                test_dataset=test_dataset_final,\n",
    "                dataloader_config=test_loader_config,\n",
    "                use_oversampler=False\n",
    "            )\n",
    "\n",
    "            if test_loader_final:\n",
    "                # --- Load Best Model ---\n",
    "                print(f\"\\nLoading best model from: {best_model_path_to_load}\")\n",
    "                # Ensure training_config, device, model_utils available\n",
    "                device = training_config.get('device')\n",
    "                \n",
    "                # Corrected line: unpack the tuple returned by model_utils.get_model\n",
    "                # We only need the model object, so use '_' for the second returned value (fast_backbone_param_names)\n",
    "                inference_model, _ = model_utils.get_model(\n",
    "                    model_name=training_config['model'], num_classes=num_classes,\n",
    "                    dropout_rate=training_config['dropout'], drop_path_rate=training_config['drop_path_rate'],\n",
    "                    finetune_blocks=training_config['finetune_blocks'], pretrained=False\n",
    "                )\n",
    "                inference_model = inference_model.to(device) # Move the model to the device\n",
    "                \n",
    "                checkpoint = torch.load(best_model_path_to_load, map_location=device)\n",
    "                state_dict = checkpoint.get('model_state_dict', checkpoint)\n",
    "                if list(state_dict.keys())[0].startswith('module.'): state_dict = {k[len('module.'):]: v for k, v in state_dict.items()}\n",
    "                inference_model.load_state_dict(state_dict)\n",
    "                inference_model.eval()\n",
    "                print(\"Model weights loaded successfully and set to eval() mode.\")\n",
    "\n",
    "                # --- Run Inference ---\n",
    "                # Ensure train_utils available and run_inference works\n",
    "                test_preds, test_labels = train_utils.run_inference(\n",
    "                    model=inference_model, dataloader=test_loader_final, device=device,\n",
    "                    num_classes=num_classes, class_names=final_class_names,\n",
    "                    description=\"Test Set Inference\"\n",
    "                )\n",
    "\n",
    "                # Check if inference returned valid results\n",
    "                if test_preds is not None and test_labels is not None:\n",
    "                    # --- Calculate and Print Metrics ---\n",
    "                    print(\"\\nCalculating test metrics...\")\n",
    "                    # Ensure train_utils available and calculate_and_print_metrics works\n",
    "                    test_metrics = train_utils.calculate_and_print_metrics(\n",
    "                        labels=test_labels, preds=test_preds, num_classes=num_classes,\n",
    "                        class_names=final_class_names, results_title=\"Final Test Set Metrics\"\n",
    "                    )\n",
    "                    # --- Save Test Metrics ---\n",
    "                    # Create a dedicated path for test metrics using the same pattern\n",
    "                    test_metrics_path = os.path.join(CHECKPOINT_DIR, f\"{file_prefix}_test_metrics.json\")\n",
    "                    print(f\"\\nAttempting to save test metrics to: {test_metrics_path}\")\n",
    "                    if test_metrics:\n",
    "                        try:\n",
    "                            # Convert numpy types before saving\n",
    "                            if 'confusion_matrix' in test_metrics and isinstance(test_metrics['confusion_matrix'], np.ndarray): \n",
    "                                test_metrics['confusion_matrix'] = test_metrics['confusion_matrix'].tolist()\n",
    "                            for key in ['binary_tn', 'binary_fp', 'binary_fn', 'binary_tp']:\n",
    "                                if key in test_metrics: test_metrics[key] = int(test_metrics[key])\n",
    "                            with open(test_metrics_path, 'w') as f: \n",
    "                                json.dump(test_metrics, f, indent=2)\n",
    "                            print(f\"  Test metrics saved successfully.\")\n",
    "                        except Exception as e: \n",
    "                            print(f\"  Warning: Could not save test metrics to JSON: {e}\")\n",
    "                    else: \n",
    "                        print(\"  Skipping test metrics saving (metrics dictionary not available).\")\n",
    "                else:\n",
    "                    print(\"\\nSkipping metrics calculation and saving (inference failed to return results).\")\n",
    "            else:\n",
    "                print(\"\\nSkipping test inference: Failed to create test DataLoader.\")\n",
    "        elif not test_dataset_final:\n",
    "            print(\"\\nSkipping test inference: Failed to load test dataset.\")\n",
    "        else:\n",
    "            print(\"\\nSkipping test inference: Number of classes is zero or invalid.\")\n",
    "    except FileNotFoundError as fnf_err:\n",
    "        print(f\"\\nSkipping test inference due to FileNotFoundError: {fnf_err}\")\n",
    "    except KeyError as key_err:\n",
    "        print(f\"\\nSkipping test inference due to KeyError: {key_err}\")\n",
    "    except AttributeError as attr_err:\n",
    "        print(f\"\\nSkipping test inference due to AttributeError: {attr_err}. Check if utility functions exist.\")\n",
    "        traceback.print_exc() # Added to help debug the actual AttributeError\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred during test inference: {e}\")\n",
    "        traceback.print_exc()\n",
    "# Handle prerequisite failures\n",
    "elif not test_data_configured:\n",
    "    print(\"\\nSkipping test inference: Test data not configured in DATA_PATHS.\")\n",
    "elif not best_model_exists:\n",
    "    print(f\"\\nSkipping test inference: Best model file not found at {best_model_path_to_load} (Likely no improvement or file path issue).\\n\")\n",
    "    print(f\"  Expected path: {best_model_path_to_load}\")\n",
    "elif not test_csv_exists:\n",
    "    print(f\"\\nSkipping test inference: Test CSV file missing or not found ({test_csv_file}).\")\n",
    "elif not test_dir_exists:\n",
    "    print(f\"\\nSkipping test inference: Test root directory missing or invalid ({test_root_dir}).\")\n",
    "\n",
    "\n",
    "# Clean up:\n",
    "del test_loader_final\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n🏁 Test Set Evaluation Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Comprehensive Training Summary\n",
    "\n",
    "This final section compiles a complete summary of the training run, including configuration parameters, best performance metrics, history highlights, dataset information, and file paths. This comprehensive report is exported to a YAML file, providing a single, human-readable record of the entire experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📝 Exporting Comprehensive Training Summary\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Start with a deep copy of the `training_config` which contains most initial parameters\n",
    "full_summary = copy.deepcopy(training_config)\n",
    "\n",
    "# Add best performance metrics from the training loop\n",
    "full_summary['best_metrics_achieved'] = {\n",
    "    'f1_score': best_metrics_summary['f1'],\n",
    "    'accuracy': best_metrics_summary['acc'],\n",
    "    'val_loss': best_metrics_summary['loss'],\n",
    "    'best_epoch': best_metrics_summary['epoch']\n",
    "}\n",
    "\n",
    "# Add a summary of training history (last and best values for key metrics)\n",
    "if history:\n",
    "    history_summary = {}\n",
    "    for key, values in history.items():\n",
    "        if len(values) > 0:\n",
    "            history_summary[f\"{key}_final\"] = values[-1] # Last recorded value\n",
    "            if key in ['val_acc', 'precision', 'recall', 'f1']:\n",
    "                history_summary[f\"{key}_best\"] = max(values) # Max for performance metrics\n",
    "            elif key in ['train_loss', 'val_loss']:\n",
    "                history_summary[f\"{key}_best\"] = min(values) # Min for loss metrics\n",
    "    full_summary['history_summary'] = history_summary\n",
    "else:\n",
    "    full_summary['history_summary'] = \"No training history available.\"\n",
    "\n",
    "# Add detailed dataset information\n",
    "full_summary['dataset_info'] = {\n",
    "    'train_csv_file': DATA_PATHS.get('train_csv_file', 'not_configured'),\n",
    "    'val_csv_file': DATA_PATHS.get('val_csv_file', 'not_configured'),\n",
    "    'test_csv_file': DATA_PATHS.get('test_csv_file', 'not_configured'),\n",
    "    'train_root_dir': DATA_PATHS.get('train_root_dir', 'not_configured'),\n",
    "    'val_root_dir': DATA_PATHS.get('val_root_dir', 'not_configured'),\n",
    "    'test_root_dir': DATA_PATHS.get('test_root_dir', 'not_configured'),\n",
    "    'initial_train_size': training_config.get('original_train_set_size', 0),\n",
    "    'resampled_train_size': training_config.get('resampled_train_set_size', 0),\n",
    "    'val_size': training_config.get('val_set_size', 0),\n",
    "    'test_size': len(test_dataset_final) if 'test_dataset_final' in globals() and test_dataset_final else 0,\n",
    "    'num_classes': num_classes,\n",
    "    'class_names': training_config.get('class_names', [])\n",
    "}\n",
    "\n",
    "# Add data transforms configuration (directly from the config dict)\n",
    "full_summary['data_transforms_config'] = DATA_TRANSFORM_CONFIG\n",
    "\n",
    "# Add dataloader configuration (directly from the config dict)\n",
    "full_summary['dataloader_config'] = DATALOADER_CONFIG\n",
    "\n",
    "# Add normalization information (directly from the config dict)\n",
    "full_summary['normalization_values'] = {\n",
    "    'mean': NORMALIZATION['mean'],\n",
    "    'std': NORMALIZATION['std']\n",
    "}\n",
    "\n",
    "# Add model configuration details\n",
    "full_summary['model_configuration'] = {\n",
    "    'name': training_config.get('model', 'unknown'),\n",
    "    'dropout': training_config.get('dropout', 0.0),\n",
    "    'drop_path_rate': training_config.get('drop_path_rate', 0.0),\n",
    "    'finetune_blocks': training_config.get('finetune_blocks', -1),\n",
    "    'pretrained_on_imagenet': True, # Assuming models are pretrained unless specified otherwise\n",
    "    'optimizer_type': training_config.get('optimizer_type', 'unknown'),\n",
    "    'scheduler_type': training_config.get('scheduler_type', 'None'),\n",
    "    'loss_function_type': training_config.get('loss_function_type', 'unknown')\n",
    "}\n",
    "\n",
    "# Add file paths for all generated outputs\n",
    "full_summary['output_file_paths'] = {\n",
    "    'base_checkpoint_dir': CHECKPOINT_DIR,\n",
    "    'best_model_checkpoint': best_model_save_path,\n",
    "    'training_history_json': history_json_path,\n",
    "    'metrics_plot_png': metrics_plot_path,\n",
    "    'learning_rate_plot_png': lr_plot_path,\n",
    "    'gpu_memory_plot_png': gpu_plot_path,\n",
    "    'run_summary_yaml': summary_yaml_path,\n",
    "    'validation_report_csv': validation_report_csv_path,\n",
    "    'test_metrics_json': os.path.join(CHECKPOINT_DIR, f\"{file_prefix}_test_metrics.json\") if 'test_metrics' in locals() else None # Only add if test was run and metrics generated\n",
    "}\n",
    "\n",
    "# Add runtime information\n",
    "full_summary['runtime_info'] = {\n",
    "    'date_completed': datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    'device': str(device),\n",
    "    'pytorch_version': torch.__version__,\n",
    "    'peak_gpu_memory_gb': max(history.get('gpu_mem_used', [0])) if 'gpu_mem_used' in history else 0.0,\n",
    "    'epochs_completed': training_config.get('epochs_completed', 0),\n",
    "    'early_stopping_triggered': training_config.get('patience_counter', 0) >= training_config.get('patience', 15)\n",
    "}\n",
    "\n",
    "# Define a recursive function to sanitize numpy types for YAML serialization\n",
    "def sanitize_for_yaml(obj):\n",
    "    if isinstance(obj, (np.int_, np.intc, np.intp, np.int8, np.int16, np.int32, np.int64, \n",
    "                        np.uint8, np.uint16, np.uint32, np.uint64)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.float16, np.float32, np.float64)):\n",
    "        if np.isnan(obj): return '.nan'\n",
    "        if np.isinf(obj): return '.inf' if obj > 0 else '-.inf'\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.bool_)):\n",
    "        return bool(obj)\n",
    "    elif isinstance(obj, (np.ndarray,)):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: sanitize_for_yaml(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [sanitize_for_yaml(item) for item in obj]\n",
    "    elif isinstance(obj, tuple):\n",
    "        return str(obj) # Represent tuples as strings\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Export the comprehensive summary to a YAML file\n",
    "comprehensive_yaml_path = os.path.join(CHECKPOINT_DIR, f\"{file_prefix}_comprehensive_summary.yaml\")\n",
    "try:\n",
    "    sanitized_summary = sanitize_for_yaml(full_summary)\n",
    "    with open(comprehensive_yaml_path, 'w') as f:\n",
    "        yaml.dump(sanitized_summary, f, default_flow_style=False, sort_keys=False)\n",
    "    print(f\"✅ Comprehensive model summary exported to: {comprehensive_yaml_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error exporting comprehensive model summary: {e}\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"🏁 Notebook Execution Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (CNN)",
   "language": "python",
   "name": "cnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
